\begin{answer}
    \begin{enumerate}
        \item No. The constant learning rate does not help converge.
        \item Yes. The learning rate is decreasing, so the model might converge when the update is smaller than the threshold.
        \item No. This is the same as scaling the theta, which does not help.
        \item Yes. This will make theta converge without being unexpectedly large, which helps stops the infinite loop.
        \item Probably. If the added noise makes the data not linearly separable, then the model will converge like A.
    \end{enumerate}
\end{answer}
