\begin{answer}

The cost function is
\begin{align*}
    J(\theta) &= \frac{1}{2} \sum_{i=1}^n \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 \\
    &= \frac{1}{2} \sum_{i=1}^n \left( \theta^T \hat{x}^{(i)} - y^{(i)} \right)^2
\end{align*}

The update rule for batch gradient descent is
\begin{align*}
    \theta_j &:= \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) \\
    &= \theta_j - \alpha \sum_{i=1}^m \left( \theta^T \hat{x}^{(i)} - y^{(i)} \right) \hat{x}_j^{(i)} \\
    \theta &:= \theta - \alpha \sum_{i=1}^m \left( \theta^T \hat{x}^{(i)} - y^{(i)} \right) \hat{x}^{(i)}
\end{align*}
where $m$ is the batch size.
\end{answer}
